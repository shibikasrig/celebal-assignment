# -*- coding: utf-8 -*-
"""house price prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MqS7eY0bSbVF4AEcNUPkK-qc68MPooUO
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder

import warnings
warnings.filterwarnings('ignore')

train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

print(f"Train shape: {train.shape}")
print(f"Test shape: {test.shape}")

missing = train.isnull().sum()
missing[missing > 0].sort_values(ascending=False).head(10)

sns.histplot(train['SalePrice'], kde=True)
plt.title("Sale Price Distribution")
plt.show()

# Only use numeric columns for correlation
numeric_cols = train.select_dtypes(include=[np.number])

# Calculate correlation
corr = numeric_cols.corr()

# Visualize correlation with SalePrice
plt.figure(figsize=(12, 8))
sns.heatmap(corr[['SalePrice']].sort_values(by='SalePrice', ascending=False), annot=True, cmap='coolwarm')
plt.title("Top Correlated Features with SalePrice")
plt.show()

train['source'] = 'train'
test['source'] = 'test'
test['SalePrice'] = np.nan
data = pd.concat([train, test])

num_cols = data.select_dtypes(include=np.number).columns
for col in num_cols:
    data[col].fillna(data[col].median(), inplace=True)

cat_cols = data.select_dtypes(include='object').columns
for col in cat_cols:
    data[col].fillna(data[col].mode()[0], inplace=True)

le = LabelEncoder()
for col in cat_cols:
    data[col] = le.fit_transform(data[col])

train = data[data['source'] == 'train'].drop(['source'], axis=1)
test = data[data['source'] == 'test'].drop(['source', 'SalePrice'], axis=1)

X = train.drop(['SalePrice', 'Id'], axis=1)
y = train['SalePrice']

print(X.shape)
print(y.shape)

data = pd.concat([train, test])

# Load again fresh (if needed)
train_raw = pd.read_csv("train.csv")
test_raw = pd.read_csv("test.csv")

# Add source column
train_raw['source'] = 'train'
test_raw['source'] = 'test'
test_raw['SalePrice'] = np.nan  # fill SalePrice for test

# Combine datasets
full_data = pd.concat([train_raw, test_raw], axis=0)

# Preprocess all features (fill missing, encode, etc. same as before)...

# Split again AFTER preprocessing
train = full_data[full_data['source'] == 'train'].drop(['source'], axis=1)
test = full_data[full_data['source'] == 'test'].drop(['source', 'SalePrice'], axis=1)

X = train.drop(['Id', 'SalePrice'], axis=1)
y = train['SalePrice']

object_cols = full_data.select_dtypes(include='object').columns
print("Categorical columns:\n", object_cols)

from sklearn.preprocessing import LabelEncoder

label_enc = LabelEncoder()
for col in object_cols:
    full_data[col] = label_enc.fit_transform(full_data[col].astype(str))

train = full_data[full_data['source'] == 'train'].drop(['source'], axis=1)
test = full_data[full_data['source'] == 'test'].drop(['source', 'SalePrice'], axis=1)

X = train.drop(['Id', 'SalePrice'], axis=1)
y = train['SalePrice']

# Fill missing numerical values with median
for col in full_data.select_dtypes(include=['number']).columns:
    full_data[col].fillna(full_data[col].median(), inplace=True)

# Fill categorical (object) columns with mode and encode them
from sklearn.preprocessing import LabelEncoder

for col in full_data.select_dtypes(include='object').columns:
    full_data[col].fillna(full_data[col].mode()[0], inplace=True)
    le = LabelEncoder()
    full_data[col] = le.fit_transform(full_data[col].astype(str))

train = full_data[full_data['source'] == 1]  # LabelEncoder will turn 'train'/'test' into 1/0
test = full_data[full_data['source'] == 0]

X = train.drop(['Id', 'SalePrice', 'source'], axis=1)
y = train['SalePrice']

X_test = test.drop(['Id', 'SalePrice', 'source'], axis=1)

print("X shape:", X.shape)
print("y shape:", y.shape)

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X, y)

# Predict
preds = rf.predict(X_test)

# Cross-validation RMSE
scores = cross_val_score(rf, X, y, scoring='neg_root_mean_squared_error', cv=5)
print(f"Random Forest CV RMSE: {-scores.mean():.2f}")